{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Regular Expression and nltk Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'write', 'RegEx']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATTERN = '\\w+'\n",
    "my_string = \"Let's write RegEx!\"\n",
    "re.findall(PATTERN, my_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Endings, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK (Natural Language Toolkit)\n",
    "sent_tokenize\n",
    "<br>word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/becode/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARTHUR', ':', 'It', 'is', 'I', ',', 'Arthur', ',', 'son', 'of', 'Uther', 'Pendragon', ',', 'from', 'the', 'castle', 'of', 'Camelot', '.']\n",
      "*************************\n",
      "{'does', 'right', 'second', 'one', 'beat', 'question', 'yet', 'that', 'if', 'non-migratory', 'there', 'needs', 'maintain', 'where', 'seek', 'to', 'them', 'all', 'south', '2', 'son', 'What', 'a', 'yeah', 'through', 'Where', 'trusty', 'fly', 'they', 'suggesting', 'length', '--', 'That', 'court', 'halves', 'could', 'who', 'In', 'our', \"'em\", 'carry', 'matter', \"'m\", 'SOLDIER', 'me', '!', 'So', 'clop', 'will', 'Arthur', 'at', 'get', 'it', 'master', 'bird', 'But', 'I', 'Please', 'maybe', 'Pull', 'is', 'other', 'my', 'then', 'agree', 'horse', 'Mercea', 'from', 'Ridden', 'may', '.', 'King', 'covered', 'ounce', 'ratios', 'Wait', '#', '?', 'Saxons', 'kingdom', 'point', 'wants', 'empty', 'times', 'Listen', 'tell', 'carrying', 'grips', 'European', ':', 'got', 'migrate', 'Halt', 'he', 'an', 'You', 'these', 'weight', 'anyway', 'Will', \"'d\", 'every', 'defeator', 'search', 'Court', 'goes', ']', 'mean', 'swallows', 'wings', 'since', 'by', 'on', 'ARTHUR', \"'s\", 'Am', 'but', 'breadth', 'not', 'Pendragon', 'be', 'tropical', 'It', 'am', 'pound', 'sovereign', 'in', \"'\", 'The', 'minute', '1', 'grip', 'SCENE', 'together', 'forty-three', 'carried', 'warmer', 'strangers', 'No', 'wind', 'you', 'order', 'Patsy', 'its', 'join', 'lord', 'winter', 'must', 'the', 'house', 'Camelot', 'Not', 'Well', 'speak', 'using', 'ridden', 'castle', 'back', \"'ve\", 'climes', '...', 'interested', 'Supposing', 'velocity', 'temperate', 'air-speed', 'sun', 'Found', 'knights', 'your', 'A', 'or', 'African', 'Whoa', 'land', 'swallow', 'servant', 'KING', 'with', 'here', 'bangin', '[', 'five', 'martin', 'bring', 'Britons', 'coconuts', 'two', 'England', 'are', 'of', 'Who', 'Oh', 'Uther', 'course', 'They', 'plover', 'have', 'ask', \"'re\", 'and', \"n't\", 'zone', 'this', 'husk', 'We', 'simple', 'go', 'line', 'Yes', 'Are', 'do', 'coconut', ',', 'found', 'snows'}\n"
     ]
    }
   ],
   "source": [
    "scene_one = \"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  \"\n",
    "\n",
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "print(tokenized_sent)\n",
    "print('*************************')\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re.search\n",
    "start() and end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n"
     ]
    }
   ],
   "source": [
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Find the script notation (e.g. Character:) at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk -> regexp_tokenize, TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    "         '#NLP is super fun! <3 #learning',\n",
    "         'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@datacamp', '#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-ascii tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text = 'Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'ðŸ•', 'Und', 'fÃ¤hrst', 'du', 'mit', 'Ãœber', '?', 'ðŸš•']\n",
      "['Wann', 'Pizza', 'Und', 'Ãœber']\n",
      "['ðŸ•', 'ðŸš•']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÃœ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charting practice with tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = scene_one.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "# Use re.sub() inside a list comprehension to replace the prompts such as ARTHUR: and SOLDIER #1. \n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)Simple topic identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = '\\'\\'\\'Debugging\\'\\'\\' is the process of finding and resolving of defects that prevent correct operation of computer software or a system.  \\n\\nNumerous books have been written about debugging (see below: #Further reading|Further reading), as it involves numerous aspects, including interactive debugging, control flow, integration testing, Logfile|log files, monitoring (Application monitoring|application, System Monitoring|system), memory dumps, Profiling (computer programming)|profiling, Statistical Process Control, and special design tactics to improve detection while simplifying changes.\\n\\nOrigin\\nA computer log entry from the Mark&nbsp;II, with a moth taped to the page\\n\\nThe terms \"bug\" and \"debugging\" are popularly attributed to Admiral Grace Hopper in the 1940s.[http://foldoc.org/Grace+Hopper Grace Hopper]  from FOLDOC While she was working on a Harvard Mark II|Mark II Computer at Harvard University, her associates discovered a moth stuck in a relay and thereby impeding operation, whereupon she remarked that they were \"debugging\" the system. However the term \"bug\" in the meaning of technical error dates back at least to 1878 and Thomas Edison (see software bug for a full discussion), and \"debugging\" seems to have been used as a term in aeronautics before entering the world of computers. Indeed, in an interview Grace Hopper remarked that she was not coining the term{{Citation needed|date=July 2015}}. The moth fit the already existing terminology, so it was saved.  A letter from J. Robert Oppenheimer (director of the WWII atomic bomb \"Manhattan\" project at Los Alamos, NM) used the term in a letter to Dr. Ernest Lawrence at UC Berkeley, dated October 27, 1944,http://bancroft.berkeley.edu/Exhibits/physics/images/bigscience25.jpg regarding the recruitment of additional technical staff.\\n\\nThe Oxford English Dictionary entry for \"debug\" quotes the term \"debugging\" used in reference to airplane engine testing in a 1945 article in the Journal of the Royal Aeronautical Society. An article in \"Airforce\" (June 1945 p.&nbsp;50) also refers to debugging, this time of aircraft cameras.  Hopper\\'s computer bug|bug was found on September 9, 1947. The term was not adopted by computer programmers until the early 1950s.\\nThe seminal article by GillS. Gill, [http://www.jstor.org/stable/98663 The Diagnosis of Mistakes in Programmes on the EDSAC], Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, Vol. 206, No. 1087 (May 22, 1951), pp. 538-554 in 1951 is the earliest in-depth discussion of programming errors, but it does not use the term \"bug\" or \"debugging\".\\nIn the Association for Computing Machinery|ACM\\'s digital library, the term \"debugging\" is first used in three papers from 1952 ACM National Meetings.Robert V. D. Campbell, [http://portal.acm.org/citation.cfm?id=609784.609786 Evolution of automatic computation], Proceedings of the 1952 ACM national meeting (Pittsburgh), p 29-32, 1952.Alex Orden, [http://portal.acm.org/citation.cfm?id=609784.609793 Solution of systems of linear inequalities on a digital computer], Proceedings of the 1952 ACM national meeting (Pittsburgh), p. 91-95, 1952.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization to create bag of words and Lowercasing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 40), ('the', 31), ('.', 19), ('of', 18), (\"''\", 14), ('in', 14), ('a', 13), ('``', 12), ('debugging', 10), ('(', 9)]\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming (to shorten words to their root stems) and Removing stop words, punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS\n",
    "english_stops = set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/becode/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 10), ('term', 9), ('computer', 8), ('system', 4), ('bug', 4), ('hopper', 4), ('used', 4), ('moth', 3), ('grace', 3), ('article', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we remove stopwords and punctuations, important words for articles remains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Gensim and Word vectors for Topic identification (based on term frequencies.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url1 = 'https://en.wikipedia.org/wiki/French_Revolution'\n",
    "url2 = 'https://en.wikipedia.org/wiki/Cartoon'\n",
    "url3 = 'https://en.wikipedia.org/wiki/English_Revolution'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r1 = requests.get(url1)\n",
    "r2 = requests.get(url2)\n",
    "r3 = requests.get(url3)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc1 = r1.text\n",
    "html_doc2 = r2.text\n",
    "html_doc3 = r3.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup1 = BeautifulSoup(html_doc1)\n",
    "soup2 = BeautifulSoup(html_doc2)\n",
    "soup3 = BeautifulSoup(html_doc3)\n",
    "\n",
    "# Get only text: french_revolution\n",
    "french_revolution = soup1.get_text()\n",
    "cartoon = soup2.get_text()\n",
    "english_revolution = soup3.get_text()\n",
    "articles = [french_revolution, cartoon, english_revolution]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, Lowercasing, Removing puctuation,  Removing stop words, Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Total of words: 36453\n",
      "Total of words after removing punctuations: 26170\n",
      "Total of words after removing stop words: 16794\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens =[word_tokenize(article) for article in articles]\n",
    "print(len(tokens))\n",
    "print(\"Total of words: {}\".format(len(tokens[0])+len(tokens[1])+len(tokens[2])))\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [[t.lower() for t in token] for token in tokens]\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only_list = [[t for t in lower_token if t.isalpha()] for lower_token in lower_tokens]\n",
    "print(\"Total of words after removing punctuations: {}\".format(len(alpha_only_list[0])+len(alpha_only_list[1])+len(alpha_only_list[2])))\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [[t for t in alpha_only if t not in english_stops] for alpha_only in alpha_only_list]\n",
    "print(\"Total of words after removing stop words: {}\".format(len(no_stops[0])+len(no_stops[1])+len(no_stops[2])))\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "articles_lemmatized = [[wordnet_lemmatizer.lemmatize(t) for t in no_stop] for no_stop in no_stops]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3347\n",
      "revolution\n",
      "revolution\n"
     ]
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles_lemmatized)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "revolution_id = dictionary.token2id.get(\"revolution\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(revolution_id)\n",
    "print(dictionary.get(revolution_id))\n",
    "print(dictionary[3347])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[(30, 1), (32, 1), (42, 2), (53, 1), (56, 1), (75, 1), (80, 1), (86, 1), (88, 1), (105, 1)]\n",
      "(3347, 344)\n"
     ]
    }
   ],
   "source": [
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles_lemmatized]\n",
    "\n",
    "# How many article\n",
    "print(len(corpus))\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the second document\n",
    "print(corpus[1][:10])\n",
    "\n",
    "# Print revolution id and its frequency count - french revolution is first document.\n",
    "print(corpus[0][3347])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can use your dictionary to look up the terms. Take a guess at what the topics are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revolution 344\n",
      "french 339\n",
      "pp 136\n",
      "*********\n",
      "cartoon 86\n",
      "comic 29\n",
      "political 19\n",
      "*********\n",
      "revolution 37\n",
      "english 28\n",
      "civil 10\n",
      "*********\n"
     ]
    }
   ],
   "source": [
    "# print top 3 words for each document: doc\n",
    "for doc in corpus:\n",
    "\n",
    "    # Sort the doc for word frequency: bow_doc\n",
    "    bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "    # Print the top 3 words of the document alongside the count\n",
    "    for word_id, word_count in bow_doc[:3]:\n",
    "        print(dictionary.get(word_id), word_count)\n",
    "    print('*********')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 1), (9, 2), (25, 1), (29, 1), (32, 1), (42, 1), (45, 1), (46, 1), (56, 1), (60, 1)]\n",
      "[(3347, 37), (1315, 28), (671, 10), (2882, 9), (4183, 9), (622, 7), (2447, 7), (458, 6), (685, 6), (1239, 6)]\n"
     ]
    }
   ],
   "source": [
    "print(doc[0:10])\n",
    "print(bow_doc[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revolution 381\n",
      "french 344\n",
      "pp 138\n",
      "history 106\n",
      "de 99\n"
     ]
    }
   ],
   "source": [
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count \n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf -> to determine the most important in each document\n",
    "If I am astronomer, sky might be used often but it is not important. Tf-idf helps keep the document-specific frequent weighted high and the common words across the entire corpus weighted low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our compus contains of 3 article. Only 2 article mentions \"revolution\". We want to find the tf-idf weight of \"revolution\" word in french revolution in this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of words are 4316 in French revolution and revolution repeats 344 times\n"
     ]
    }
   ],
   "source": [
    "print('Total count of words are {} in French revolution and revolution repeats {} times'.format(len(corpus[0]), corpus[0][3347][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In math, tf-idf for revolution in French revolution is   (344/4316) * log(3/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03231695949703627\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "result = (344/4316)*(math.log(3/2))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3998604516989592\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# select the fench revolution document: doc\n",
    "doc = corpus[0]\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# revolotuin id is 3347\n",
    "for i in tfidf_weights:\n",
    "    if 3347 in i:\n",
    "        print(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revolution 37\n",
      "english 28\n",
      "civil 10\n",
      "period 9\n",
      "war 9\n",
      "*********\n",
      "[(3347, 37), (1315, 28), (671, 10), (2882, 9), (4183, 9)]\n",
      "*********\n",
      "[(3347, 0.6325740441972113), (671, 0.17096595789113816), (2447, 0.11967617052379673), (458, 0.10257957473468292), (685, 0.10257957473468292)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Save the fench revolution document: doc\n",
    "doc = corpus[2]\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "\n",
    "# Sort the doc for word frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "print('*********')\n",
    "print(bow_doc[0:5]) \n",
    "print('*********')\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[bow_doc]\n",
    "\n",
    "# Print the revolution weight: revolution id-> five weights\n",
    "print(tfidf_weights[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
